{% set name = "apache-airflow-providers-apache-hdfs" %}
{% set version = "2.2.1" %}


package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/apache-airflow-providers-apache-hdfs-{{ version }}.tar.gz
  sha256: 8f67982e95dffeba72c970014a4f2f170228dd0a8be1a0915acc6a9ecff0d48f

build:
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv

requirements:
  host:
    - python >=3.7
    - pip
    - setuptools
  run:
    - python >=3.7
    - apache-airflow >=2.1.0
    # conda python-hdfs package includes dependencies for avro, dataframe, and kerberos extras
    - python-hdfs >=2.0.4
    - snakebite-py3
test:
  imports:
    - airflow.providers.apache.hdfs
    - airflow.providers.apache.hdfs.hooks
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://airflow.apache.org/
  summary: Provider for Hadoop Distributed File System (HDFS) and WebHDFS for Apache Airflow
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  doc_url: https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/index.html
  dev_url: https://github.com/apache/airflow/

extra:
  recipe-maintainers:
    - xylar
